{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0901a34",
   "metadata": {},
   "source": [
    "schema design:\n",
    "\n",
    "Article(\"ar_ID\", title, url, type, scraped_at, updated_at)\n",
    "AuthorTo(Name,\"ar_ID\")\n",
    "KeywordTo(\"keyword\", \"article\")\n",
    "WordIn(\"word\", \"article\", amount)\n",
    "\n",
    "do we db-ify vocab? [yes]\n",
    "do we add value checks? nah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a555f",
   "metadata": {},
   "source": [
    "Creation:\n",
    "\n",
    "    CREATE TABLE Article (\n",
    "        a_id INT PRIMARY KEY,\n",
    "        title CHAR(255),\n",
    "        url CHAR(128),\n",
    "        author CHAR(64),\n",
    "        type CHAR(16),\n",
    "        scraped_at TIMESTAMP,\n",
    "        inserted_at TIMESTAMP,\n",
    "        updated_at TIMESTAMP,\n",
    "        content TEXT,\n",
    "        summary TEXT\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Includes_keyword(\n",
    "        article INT,\n",
    "        keyword CHAR(32),\n",
    "        PRIMARY KEY (article, keyword)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Includes_word(\n",
    "        article INT,\n",
    "        word CHAR(47),\n",
    "        count INT,\n",
    "        PRIMARY KEY (article, word)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE Includes_author(\n",
    "        article INT,\n",
    "        name CHAR(32),\n",
    "        PRIMARY KEY (article, name)\n",
    "    );\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ad405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('PostgreSQL 13.6 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.2.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(\"dbname=m2DATA user=postgres password=posty\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute('SELECT version()')\n",
    "    print(cur.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fa113",
   "metadata": {},
   "source": [
    "Here we include all the code from last milestone for cleainging the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "321e036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68763911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2499/1701869104.py:2: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfRaw = pd.read_csv ('1mio-raw.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>express.co.uk</td>\n",
       "      <td>rumor</td>\n",
       "      <td>Life is an illusion, at least on a quantum lev...</td>\n",
       "      <td>Is life an ILLUSION? Researchers prove 'realit...</td>\n",
       "      <td>Sean Martin</td>\n",
       "      <td>['']</td>\n",
       "      <td>THE UNIVERSE ceases to exist when we are not l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>Unfortunately, he hasn’t yet attacked her for ...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>barenakedislam.com</td>\n",
       "      <td>hate</td>\n",
       "      <td>The Los Angeles Police Department has been den...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id              domain   type  \\\n",
       "0  2       express.co.uk  rumor   \n",
       "1  6  barenakedislam.com   hate   \n",
       "2  7  barenakedislam.com   hate   \n",
       "\n",
       "                                             content  \\\n",
       "0  Life is an illusion, at least on a quantum lev...   \n",
       "1  Unfortunately, he hasn’t yet attacked her for ...   \n",
       "2  The Los Angeles Police Department has been den...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Is life an ILLUSION? Researchers prove 'realit...   \n",
       "1                                       Donald Trump   \n",
       "2                                       Donald Trump   \n",
       "\n",
       "                                             authors meta_keywords  \\\n",
       "0                                        Sean Martin          ['']   \n",
       "1  Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...          ['']   \n",
       "2  Linda Rivera, Conrad Calvano, Az Gal, Lincoln ...          ['']   \n",
       "\n",
       "                                    meta_description tags  source  \n",
       "0  THE UNIVERSE ceases to exist when we are not l...  NaN     NaN  \n",
       "1                                                NaN  NaN     NaN  \n",
       "2                                                NaN  NaN     NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Parse csv into dataframe\n",
    "dfRaw = pd.read_csv ('1mio-raw.csv')\n",
    "\n",
    "# drop some tables for brevity:\n",
    "df = dfRaw.iloc[: , 1:]\n",
    "df = df.drop(columns =[ \"url\", \"scraped_at\", \"scraped_at\", \"inserted_at\", \"updated_at\", \"keywords\", \"summary\"])\n",
    "\n",
    "# have a peek at the data:\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b80343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to make a pd.Series -> pd.Series function that cleans all the text\n",
    "# lowercases everything removes urls, emails etc.\n",
    "\n",
    "def clean_text(textSeries:pd.Series) -> pd.Series:\n",
    "    def clean_func (text):\n",
    "        return clean(text,\n",
    "            fix_unicode=True,            \n",
    "            to_ascii=True,               \n",
    "            lower=True,                  \n",
    "            no_line_breaks=True,        \n",
    "            no_urls=True,                \n",
    "            no_emails=True,              \n",
    "            no_phone_numbers=False,      \n",
    "            no_numbers=True,             \n",
    "            no_punct=True,              \n",
    "            replace_with_punct=\"\",       \n",
    "            replace_with_url=\"oURL\", # clean_text lowers after tagging for some reason???\n",
    "            replace_with_email=\"oEMAIL\",\n",
    "            replace_with_phone_number=\"oPHONE\",\n",
    "            replace_with_number=\"oNUM\",\n",
    "            replace_with_digit=\"0\",\n",
    "            lang=\"en\")\n",
    "    return textSeries.apply(clean_func)\n",
    "            \n",
    "df.content = clean_text(df.content)\n",
    "df[\"meta_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28560dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "nltk.download('punkt') # fix\n",
    "\n",
    "# Same as before a pd.Series -> pd.Series function.\n",
    "def tokenize(textSeries:pd.Series) -> pd.Series:\n",
    "    return pd.Series(textSeries.apply(nltk.tokenize.wordpunct_tokenize),name=\"tokens\")\n",
    "df[\"tokens\"] = tokenize(df.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282ee77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# stemming\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# stems all the tokens\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem\u001b[39m (data:\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mSeries)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m      4\u001b[0m     ps \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# function to be applied on each element in series\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "# stems all the tokens\n",
    "def stem (data:pd.Series)-> pd.Series:\n",
    "    ps = PorterStemmer()\n",
    "    # function to be applied on each element in series\n",
    "    def stem (tokens):\n",
    "        return [ps.stem(token) for token in tokens]\n",
    "    return pd.Series(data.apply(stem))\n",
    "\n",
    "# removal of duplicates\n",
    "def dupeRemove (tokens:pd.Series) -> pd.Series:\n",
    "    def unDupe (tokens):\n",
    "        return list(set(tokens))\n",
    "    return tokens.apply(unDupe)\n",
    "\n",
    "\n",
    "\n",
    "df[\"stemmed_with_dupes\"] = stem(df.tokens)\n",
    "df[\"stemmed\"] = dupeRemove(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following section we have BOW code, to generate bags of words\n",
    "# and combining them. As previous we well be using pd.Series as out datatype.\n",
    "# The code takes a Long time to complete, might want to skip this section and \n",
    "# instead unpickle. \n",
    "\n",
    "\n",
    "# generate BOW given a set of tokens(with dupes).\n",
    "def BOWGen(tokenss:pd.Series) -> pd.DataFrame: # with 2 cols for word and count\n",
    "    \n",
    "    #generate Bow for 1 list of tokens\n",
    "    def _BOWGen(tokens:list) -> (list,list): # with 2 cols for word and count\n",
    "        vocab = []\n",
    "        freqs = []\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                # increment frequncy counter by one.\n",
    "                freqs[vocab.index(token)] += 1\n",
    "        return vocab,freqs\n",
    "    return tokenss.apply(_BOWGen)\n",
    "\n",
    "\n",
    "def BOWGenGlobal(BOWs:pd.Series) -> pd.DataFrame:\n",
    "    def mergeBOWs(a:(list,list), b):\n",
    "        vocab, freqs = a\n",
    "        for (word, count) in zip(b[0],b[1]):\n",
    "            if word not in vocab:\n",
    "                vocab.append(word)\n",
    "                freqs.append(count)\n",
    "            else:\n",
    "                freqs[vocab.index(word)] += count\n",
    "        return vocab, freqs\n",
    "    \n",
    "    BOWList = [i for (_, i) in BOWs.items()]\n",
    "    \n",
    "    # accumulator\n",
    "    _BOW = ([],[])\n",
    "    for idx, BOW in enumerate(BOWList):\n",
    "        _BOW = mergeBOWs(BOW, _BOW)\n",
    "        if (idx) % 20 == 0:\n",
    "            print(f\"{idx} out of {len(BOWList)} \")\n",
    "    out = pd.DataFrame((_BOW)).transpose()\n",
    "    out.columns = [\"word\", \"count\"]\n",
    "    out[\"count\"] = out[\"count\"].astype(int)\n",
    "    out[\"frequency\"] = out[\"count\"]/out.size\n",
    "    print(\"Done\")\n",
    "    return out\n",
    "      \n",
    "# test \n",
    "BOWs = BOWGenGlobal(BOWGen(df.tokens.iloc[0:10]))\n",
    "BOWs.sort_values(\"count\",ascending=False).iloc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbd62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb42e5d5",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Description\n",
    "\n",
    "After cleaning and processing our data in the first milestone, Milestone 2 will focus on how to efficiently represent the data in a database. Like last time, the milestone takes the form of a short jupyter notebook. It must be handed in on Friday, April 29, at 16:00 (in groups), and it is a requirement for attending the exam (it will be evaluated as passed/fail).\n",
    "\n",
    "\n",
    "## Task 1.\n",
    "\n",
    "The first task is to demonstrate that you have a working database containing the FakeNewsCorpus dataset. Explain your choice of schema design. You have been working on this task on a small subset of the data during the TA-sessions. For this milestone, demonstrate that your database contains a larger number of rows (e.g. one million - or however many you can reasonably work with on your available hardware), and that it supports simple queries.\n",
    "\n",
    "\n",
    "## Task 2.\n",
    "\n",
    "List the relations you have created in your database. For each relation:\n",
    "\n",
    "    list its attributes\n",
    "    list its functional dependencies.\n",
    "    list all the primary keys.\n",
    "\n",
    "Is each relation in BCNF form? If not, show how to transform the tables in BCNF and explain why it might be better (or not) to use the BCNF relations in your database.\n",
    "\n",
    "\n",
    "## Task 3.\n",
    "\n",
    "Once your database is loaded, you can start issuing queries to better understand the characteristics of the data. Formulate the following queries in the database languages requested (in the square brackets following each item) and briefly discuss what you observe when you execute them over your database:   \n",
    "\n",
    "    List the domains of news articles of reliable type and scraped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer. [Languages: relational algebra and SQL]\n",
    "    List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset. [Languages: extended relational algebra and SQL]\n",
    "    Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. [Language: SQL]\n",
    "\n",
    "\n",
    "## Task 4.\n",
    "\n",
    "Now that we have our data in a database, let’s revisit the “interesting observations” task from Milestone 1 - but now using queries to the database. The idea is to write database queries (e.g. using GROUP BY and COUNT) that explore features of the data set that are relevant to the fake news prediction task: outliers, artefacts. It’s OK to investigate the same issues as in Milestone 1 (now using database queries) - but you are also very welcome to come up with completely new queries. You should write at least 3 such queries.\n",
    "\n",
    "\n",
    "## Task 5.\n",
    "\n",
    "Just like last time, after the hand in deadline, each group will be asked to evaluate the work of three other groups, based on a short list of criteria that you can find within the peergrade system. Again, this will only work well if everyone puts some effort into providing constructive comments, so please allocate some time to do this properly. It is an opportunity to get some feedback that can help you improve your final project. The deadline for giving feedback is a week after the hand-in deadline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
